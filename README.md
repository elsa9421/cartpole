
In this repository we learn cartpole...

# Notebooks

* **nb1-gym-cartpolen**

This notebooks contains...

* **nb2-tql-cartpolen**
* **nb3-dql-cartpolen**

# Problems
The convergence of Table Q-learning seem to be very sensitive to the learning parameters. How can one fix this?

Why do we need this?
```
if done:  # Terminal state
    sample[3] = None
```
